# Mnemos Implementation Plan

> Parseable task checklist for the autonomous build loop.
> Format: `- [ ] P{phase}.{num}: {description}` (unchecked = pending, checked = done)

## Phase 1: Foundation

- [x] P1.1: Scaffold project structure -- Create docker-compose.yml (all 5 services: caddy, frontend, backend, qdrant, ollama), Caddyfile, .env.example, .gitignore, backend/Dockerfile, backend/requirements.txt, frontend/Dockerfile, frontend/package.json
- [x] P1.2: Backend core -- Create backend/app/__init__.py, main.py (FastAPI with CORS, lifespan), config.py (Pydantic Settings), db.py (SQLite + SQLModel engine/session), routers/health.py
- [x] P1.3: Memory model and CRUD -- Create backend/app/models/__init__.py, models/memory.py (Memory SQLModel, plaintext initially), routers/memories.py (full CRUD: POST, GET list, GET by id, PUT, DELETE)
- [x] P1.4: Frontend scaffold -- Create frontend/vite.config.ts, tailwind.config.js, tsconfig.json, index.html, src/main.tsx, src/App.tsx (React Router shell with Layout), src/components/Layout.tsx (nav sidebar), src/types/index.ts
- [x] P1.5: Frontend capture and timeline -- Create frontend/src/services/api.ts (fetch wrapper for backend), src/components/Capture.tsx (text input form), src/components/Timeline.tsx (chronological memory list), src/components/MemoryDetail.tsx (view/edit single memory)
- [x] P1.6: Phase 1 integration test -- Verify docker compose build succeeds, all services start, health endpoint returns 200, can create and list memories via API, frontend renders and connects to backend

## Phase 2: The Shield

- [x] P2.1: Encryption service -- Create backend/app/services/__init__.py, services/encryption.py (AES-256-GCM envelope encryption with DEK/KEK, HKDF key derivation, HMAC search tokens), backend/app/utils/__init__.py, utils/crypto.py (Argon2id key derivation, low-level primitives)
- [x] P2.2: Auth system -- Create backend/app/routers/auth.py (passphrase-based auth: derive master key client-side, verify via HMAC, JWT session tokens with 15min access + 7day refresh), backend/app/models/auth.py (session model), backend/app/dependencies.py (FastAPI deps for auth + encryption service injection)
- [x] P2.3: Client-side crypto -- Create frontend/src/services/crypto.ts (ClientCrypto class: Argon2id via argon2-browser WASM, HKDF key derivation, AES-256-GCM encrypt/decrypt with DEK/KEK envelope, lock/unlock/wipe), frontend/src/hooks/useEncryption.ts (key lifecycle, auto-lock on timeout), frontend/src/hooks/useAuth.ts (session management)
- [x] P2.4: Login UI and encrypted flow -- Create frontend/src/components/Login.tsx (passphrase entry, derive key, authenticate), modify Capture.tsx and Timeline.tsx to encrypt before send and decrypt after receive, add auth guards to all routes
- [x] P2.5: Shield integration test -- Verify passphrase login works end-to-end, create a memory via UI, inspect SQLite database to confirm all content fields are ciphertext (not plaintext), verify session timeout locks vault, write backend/tests/test_encryption.py

## Phase 3: The Vault

- [x] P3.1: Vault service -- Create backend/app/services/vault.py (age encryption via pyrage, file storage in data/vault/YYYY/MM/{uuid}.age, retrieve and decrypt, integrity verification with SHA-256)
- [x] P3.2: Preservation and ingestion -- Create backend/app/services/preservation.py (format conversion: JPEG→PNG, MP3→FLAC, DOCX→PDF/A+MD, HTML→MD using Pillow/ffmpeg/pandoc), backend/app/services/ingestion.py (content type detection, processing pipeline, coordinate encryption+vault+preservation)
- [x] P3.3: Ingest API and source model -- Create backend/app/models/source.py (Source SQLModel with vault_path, mime_type, preservation_format, per-file DEK), backend/app/routers/ingest.py (POST /api/ingest multipart upload), backend/app/routers/vault.py (GET file from vault, decrypt and serve)
- [x] P3.4: Enhanced capture UI -- Modify frontend/src/components/Capture.tsx to add drag-and-drop file upload zone, voice recorder (MediaRecorder API), photo capture (camera API), URL import field. Show upload progress and preservation status.
- [x] P3.5: Vault integration test -- Verify file upload works end-to-end, uploaded JPEG is stored as data/vault/YYYY/MM/{uuid}.age, file can be decrypted and is a valid lossless PNG conversion, write backend/tests/test_vault.py and backend/tests/test_ingestion.py

## Phase 4: The Cortex

- [x] P4.1: Embedding service -- Create backend/app/services/embedding.py (text chunking with overlap, generate embeddings via Ollama nomic-embed-text, store vectors in Qdrant with encrypted chunk payloads), ensure Qdrant collection is initialized on startup
- [x] P4.2: RAG and LLM services -- Create backend/app/services/rag.py (query embedding, top-K retrieval from Qdrant, decrypt chunks, build prompt with context, generate answer via LLM), backend/app/services/llm.py (Ollama abstraction: generate, stream, model management, optional cloud API fallback)
- [x] P4.3: Connection service -- Create backend/app/services/connections.py (find similar memories via embedding similarity, ask LLM to explain relationships, create Connection records), backend/app/models/connection.py (Connection SQLModel), backend/app/services/search.py (blind index HMAC search + vector search fusion, ranked results)
- [x] P4.4: Search and connection models -- Create backend/app/models/search_token.py (SearchToken SQLModel for blind index), backend/app/routers/search.py (GET /api/search with query, filters, pagination), backend/app/routers/cortex.py (connection CRUD, trigger re-analysis)
- [x] P4.5: Chat API and worker -- Create backend/app/routers/chat.py (WebSocket endpoint for streaming RAG chat), backend/app/worker.py (background job processor: embedding generation, connection discovery, heartbeat checks on new memory ingest)
- [x] P4.6: AI frontend -- Create frontend/src/components/Chat.tsx (WebSocket chat UI with streaming responses, source citations), frontend/src/components/Search.tsx (search with filters, result cards), frontend/src/components/Graph.tsx (D3.js force-directed graph of memory connections)
- [x] P4.7: Cortex integration test -- Verify embedding pipeline works (add memories, check Qdrant has vectors), search returns results, chat answers questions with source citations, connection graph renders, write backend/tests/test_search.py

## Phase 5: The Testament

- [x] P5.1: Shamir service -- Create backend/app/services/shamir.py (SLIP-39 key splitting: 3-of-5 threshold, mnemonic word shares, reconstruct from K shares), scripts/shamir-split.py (CLI tool), scripts/shamir-combine.py (CLI tool)
- [x] P5.2: Heartbeat service -- Create backend/app/services/heartbeat.py (dead man's switch: generate challenge, verify check-in with HMAC, escalation timeline at 30/45/60/75/90 days, alert dispatch), backend/app/models/heartbeat.py (Heartbeat and HeartbeatAlert SQLModels)
- [x] P5.3: Testament API -- Create backend/app/routers/heartbeat.py (GET challenge, POST check-in, GET status), backend/app/routers/testament.py (heir configuration, Shamir share metadata, heir-mode activation with read-only access + chat)
- [x] P5.4: Testament UI -- Create frontend/src/components/Heartbeat.tsx (monthly check-in button, days remaining, alert history), frontend/src/components/Testament.tsx (Shamir share management, heir list, inheritance status), frontend/src/components/Settings.tsx (system config, encryption status, backup status)
- [x] P5.5: Testament integration test -- Verify shamir-split.py generates 5 shares, any 3 reconstruct the key, 2 shares fail, heartbeat check-in resets timer, simulated 30-day gap triggers reminder, write backend/tests/test_shamir.py and backend/tests/test_heartbeat.py

## Phase 6: Hardening

- [x] P6.1: Backup and restore -- Create scripts/backup.sh (3-2-1-1-0 with restic: SQLite safe backup, local + B2 + S3 cold, verify, prune), scripts/restore.sh (verified restore from any restic repo)
- [x] P6.2: Migration and init -- Create scripts/migrate.sh (tar + sha256 bundle, instructions for new server), scripts/init.sh (first-time setup wizard: generate salt, create .env, pull Docker images, initialize DB, pull Ollama models)
- [x] P6.3: Monitoring and Makefile -- Create scripts/health-check.sh (cron-friendly: check all services, DB integrity, vault integrity, disk space, alert on failure), Makefile (targets: up, down, backup, restore, health, logs, shell, migrate, init)
- [x] P6.4: Documentation -- Create README.md (quick start, architecture overview, deployment guide), RECOVERY.md (total reconstruction guide for Shamir share holders, step-by-step with no assumed knowledge)
- [x] P6.5: Full test suite -- Create backend/tests/conftest.py (shared fixtures: test DB, test encryption keys, mock services), expand all test files with edge cases, add integration tests that verify the full pipeline end-to-end, docker-compose.test.yml for isolated test environment

## Discovery Round 2

- [x] D2.1: Fix insecure master key wiping in auth_state.py -- `wipe_master_key()` uses `del key` which only removes the reference; the bytes remain in process memory. Must overwrite with zeros in-place (`bytearray` swap or `ctypes.memset`) before deleting. Same issue in `wipe_all()` which calls `.clear()` without zeroing values first.
- [x] D2.2: Validate encryption algo/version on decrypt in encryption.py -- `EncryptionService.decrypt()` ignores `envelope.algo` and `envelope.version`, so a future envelope with a different algorithm would silently decrypt with the wrong method. Add validation that algo/version match supported values before decrypting; raise a clear error for unsupported versions (crypto-agility gate).
- [x] D2.3: Implement URL ingestion in ingestion.py -- `ingest_url()` raises `NotImplementedError`. ARCHITECTURE.md specifies HTML/webpage ingestion via readability + pandoc conversion to Markdown. Implement using httpx for fetching and readability-lxml (or similar) for content extraction, storing both the original HTML in the vault and a Markdown extract as the memory content.
- [x] D2.4: Add retry mechanism and failure tracking to background worker -- `worker.py` silently discards failed jobs (embedding, connection discovery, search tokenization) with only a log entry. Add a job status model (pending/processing/succeeded/failed), persist failed jobs to the DB, and implement configurable retry with exponential backoff. Surface failures via the health endpoint.
- [x] D2.5: Add `nodev` to tmpfs mount and pin Ollama image version in docker-compose.yml -- tmpfs at `/app/tmp` is missing the `nodev` flag (should be `size=256m,noexec,nosuid,nodev`). Ollama uses `latest` tag which can break builds unexpectedly; pin to a specific version. Also add JSON-file logging driver with max-size/max-file limits to prevent unbounded log growth.
- [x] D2.6: Create docker-compose.prod.yml with resource limits -- ARCHITECTURE.md references `docker-compose.prod.yml` for production overrides but the file doesn't exist. Create it with CPU/memory limits per service (especially Ollama which can OOM-kill the host), restart policies, and read-only root filesystem where possible.
- [x] D2.7: Add missing test coverage for encryption roundtrip, auth flow, Shamir split/reconstruct, and connections -- Tests exist for vault, search, embedding, heartbeat, and ingestion but there are no dedicated `test_encryption.py` (envelope encrypt/decrypt roundtrip, wrong-key rejection, algo validation), `test_auth.py` (login/logout/session-timeout), `test_shamir.py` (split/reconstruct/threshold-failure), or `test_connections.py` (connection discovery, relationship extraction). Add these with edge cases.
- [x] D2.8: Implement optional cloud LLM fallback in llm.py -- There's a `TODO: optional cloud API fallback (OpenAI-compatible endpoint)` comment. When Ollama is unavailable, all AI features (chat, embeddings, connection generation) fail completely. Add config options for an OpenAI-compatible fallback endpoint (`FALLBACK_LLM_URL`, `FALLBACK_LLM_API_KEY`) with automatic failover and health-based routing.

## Discovery Round 3

- [x] D3.1: Replace assert statements with proper error handling in `backend/app/routers/ingest.py` -- Lines 221-222 and 309-310 use `assert` to validate that `title_envelope` and `content_envelope` are not None. Assertions are stripped when Python runs with `-O` (optimize) flag in production, turning these into silent no-ops. Replace with explicit `if ... is None: raise HTTPException(status_code=422, detail="Encryption envelope missing")`.
- [x] D3.2: Persist heartbeat challenges to database instead of in-memory dict -- `backend/app/services/heartbeat.py` line 44 stores pending challenges in `self._pending_challenges: dict[str, datetime]` which is lost on server restart. If the server restarts between challenge generation and check-in verification, the owner cannot complete their heartbeat. Store challenges in a database table with expiry timestamps.
- [x] D3.3: Implement `services/git_ops.py` for memory version history -- ARCHITECTURE.md §4 specifies `services/git_ops.py` for git version tracking, and `requirements.txt` includes `GitPython>=3.1`, but the file does not exist and no git operations are performed anywhere in the codebase. Memories have a `git_commit` field that is never populated. Implement git commit on memory create/update, storing the commit SHA on the Memory record.
- [x] D3.4: Create `models/tag.py` and implement tagging system -- ARCHITECTURE.md §4 specifies `models/tag.py` for tags and categories but the file does not exist. No tag model, no tag CRUD endpoints, and no tag-based filtering exists. The metadata_encrypted field could hold tags but there is no structured tagging workflow. Implement a Tag model, many-to-many relationship with Memory, and tag CRUD endpoints.
- [x] D3.5: Use a dedicated JWT secret instead of reusing `auth_salt` -- `backend/app/routers/auth.py` lines 51, 64, 71 use `settings.auth_salt` as the JWT signing secret. The auth_salt is the Argon2id salt shared with the client for key derivation; if it leaks (it's sent to the browser during login), an attacker can forge JWT sessions. Add a separate `JWT_SECRET` config value (auto-generated in `scripts/init.sh`) used only for token signing.
- [x] D3.6: Add connection explanation display to frontend Graph and MemoryDetail components -- `frontend/src/types/index.ts` defines `explanation_encrypted` and `explanation_dek` on the Connection type, and the backend returns these fields, but no frontend component ever decrypts or displays them. Users cannot see why AI connected two memories. Add explanation decryption and display on hover/click in Graph.tsx and as a detail section in MemoryDetail.tsx.
- [x] D3.7: Implement `services/backup.py` for programmatic backup orchestration -- ARCHITECTURE.md §4 specifies `services/backup.py` for backup orchestration but the file does not exist. The `scripts/backup.sh` handles backup from the host, but there is no API endpoint to trigger or monitor backups, and the health endpoint cannot report backup status. Create the service with last-backup-time tracking and a `/api/backup/status` endpoint.
- [x] D3.8: Add `nodev` flag to Qdrant and Ollama health checks and fix Alpine bash dependency -- `docker-compose.yml` lines 65 and 80 use `bash -c 'echo > /dev/tcp/...'` for health checks, but Alpine-based images (Qdrant uses Alpine) may not have bash. Replace with `wget --spider` or `curl` based checks. Also standardize health check format across all services (backend uses CMD+curl, others use CMD-SHELL+bash).

## Discovery Round 4

- [x] D4.1: Add missing `source_id` foreign key field to Memory model -- ARCHITECTURE.md §5.1 specifies `source_id: str | None = None  # FK to Source (original file in vault)` on the Memory model, but `backend/app/models/memory.py` does not have this field. This breaks the documented relationship between Memory and Source: when a file is ingested and stored in the vault, there is no way to link the Memory record back to its Source record via a direct FK. Add `source_id` field with `foreign_key="sources.id"` to Memory and corresponding fields to MemoryCreate/MemoryRead schemas.
- [x] D4.2: Enforce `MAX_UPLOAD_SIZE_MB` in frontend file upload handler -- `frontend/src/components/Capture.tsx` defines `MAX_UPLOAD_SIZE_MB = 500` (line 31) and displays it to users (line 237), but never actually validates file size before uploading. Users can attempt arbitrarily large uploads that will fail on the backend, wasting bandwidth and causing confusing errors. Add an explicit size check in `handleFileUpload()` before calling `uploadFileWithProgress()`.
- [x] D4.3: Add React ErrorBoundary to prevent full-app white-screen crashes -- No ErrorBoundary component exists anywhere in `frontend/src/`. An uncaught error in any component (e.g., a decryption failure, malformed API response, or graph rendering error) crashes the entire app with a blank white screen and no recovery path. Create an ErrorBoundary wrapper in `App.tsx` that catches render errors, displays a fallback UI with the error message, and offers a "reload" button.
- [x] D4.4: Add rate limiting to Caddyfile for API routes -- ARCHITECTURE.md §10.3 specifies "Rate limiting at Caddy level (100 req/min per IP)" but the `Caddyfile` has no rate limiting configuration. Without it, the API is vulnerable to brute-force passphrase attempts and denial-of-service. Add `rate_limit` directive to `/api/*` routes using Caddy's built-in rate limiting module.
- [x] D4.5: Add WebSocket reconnection with exponential backoff in Chat component -- `frontend/src/components/Chat.tsx` creates a WebSocket connection on mount but has no automatic reconnection logic. If the backend restarts or the network briefly drops, users must manually refresh the entire page to restore chat. Implement exponential backoff retry (1s, 2s, 4s, 8s, max 30s) with a visible "Reconnecting..." indicator and a manual "Reconnect" button.
- [x] D4.6: Add fallback LLM configuration prompts to `scripts/init.sh` -- The `.env.example` documents 4 optional cloud LLM fallback variables (`FALLBACK_LLM_URL`, `FALLBACK_LLM_API_KEY`, `FALLBACK_LLM_MODEL`, `FALLBACK_EMBEDDING_MODEL`) and `backend/app/config.py` supports them, but `scripts/init.sh` never prompts for or writes these variables during setup. Users must manually edit `.env` to enable cloud fallback. Add an optional fallback LLM configuration section to the interactive setup wizard.
- [x] D4.7: Create frontend tag management UI -- Backend has a complete tagging system (`models/tag.py`, `routers/tags.py`) and `frontend/src/types/index.ts` defines `Tag` and `MemoryTag` interfaces, but no frontend component allows users to create, assign, or filter by tags. Add tag chips to Capture.tsx (assign on create), MemoryDetail.tsx (view/edit tags), and a tag filter to Timeline.tsx and Search.tsx.

## Discovery Round 5

- [x] D5.1: Add manual captured_at date editing in MemoryDetail -- The LLM auto-extracts historical dates during ingest but may get them wrong, and there is no way for the user to correct `captured_at` from the UI. Add an editable date picker to `frontend/src/components/MemoryDetail.tsx` that lets the user view and override `captured_at`. The PUT `/api/memories/{id}` endpoint already accepts `captured_at` in the `MemoryUpdate` schema, so only frontend changes are needed: a date input field (defaulting to the current `captured_at`), a save button that calls `updateMemory()` with the new date, and optimistic UI update. After saving, refresh timeline stats if on the Timeline page.

- [x] D5.2: Add bulk file import with progress tracking -- Users need to import many old files (emails, photos, documents) at once. Add a bulk import flow: in `frontend/src/components/Capture.tsx`, allow selecting multiple files (via file picker or drag-and-drop), show a queued file list with individual progress bars, upload files sequentially (to avoid overwhelming the backend/Ollama), show per-file status (pending/uploading/processing/done/error), and a summary when complete. Reuse the existing `uploadFileWithProgress()` API function. Add a total progress indicator and the ability to cancel remaining uploads.

- [x] D5.3: Auto-refresh timeline bar after memory creation -- After uploading a new memory from the Capture page or after the background worker updates `captured_at`, the Timeline page's year bar is stale until the user manually refreshes. Fix this by: (1) refetching timeline stats when the Timeline component regains focus (via `document.addEventListener('visibilitychange')`), and (2) adding a lightweight refresh button/icon next to the timeline bar header. Do NOT use polling -- only refresh on visibility change and explicit user action.

- [x] D5.4: Make the UI responsive for mobile devices -- The current layout uses a fixed sidebar that doesn't adapt to small screens. Make the app usable on phones: (1) Convert the sidebar in `frontend/src/components/Layout.tsx` to a collapsible hamburger menu on screens below `md` breakpoint (768px). (2) In `Timeline.tsx`, make the TimelineBar horizontally scrollable on narrow screens and ensure memory cards stack properly. (3) In `Capture.tsx`, make the drag-and-drop zone and form inputs full-width on mobile. (4) In `Chat.tsx`, ensure the chat input stays fixed at the bottom on mobile keyboards. Use only Tailwind responsive prefixes (`sm:`, `md:`, `lg:`) -- no CSS modules or media queries.

## Discovery Round 6

- [x] D6.1: Add PDF text extraction to preservation service -- Currently `application/pdf` is marked as already-archival in `_ARCHIVAL_MIMES` so PDFs pass through with NO text extraction (`text_extract=None`). This means uploaded PDFs are stored in the vault but their content is never indexed, searchable, or embeddable. Fix: (1) Add `pdfplumber` to `backend/requirements.txt`. (2) In `backend/app/services/preservation.py`, remove `application/pdf` from `_ARCHIVAL_MIMES` so it enters the conversion path. (3) Add a `_extract_pdf_text()` method that uses pdfplumber to extract text from all pages, concatenating into a single string. (4) Add a PDF handler block in `convert()` (after the document block) that returns the original PDF bytes unchanged as `preserved_data` (no format conversion needed) but populates `text_extract` with the extracted text. (5) Update `PRESERVATION_MAP` entry for `application/pdf` from `"pdf"` to `"pdf+text"` to indicate text extraction is performed. (6) Add `pdfplumber` to the backend Dockerfile if not already installed. (7) Handle edge cases: encrypted/password-protected PDFs (log warning, return None text_extract), scanned image-only PDFs (text_extract will be empty string — acceptable for now, OCR is a future enhancement).

- [x] D6.2: Add legacy DOC and RTF support to preservation service -- The preservation service handles DOCX (via pandoc) but not legacy `.doc` (application/msword) or `.rtf` (application/rtf) files. Users with old documents need these formats supported. Fix: (1) Add `application/msword` and `application/rtf` entries to `PRESERVATION_MAP` with value `"pdf-a+md"`. (2) Add `.doc` and `.rtf` entries to `_MIME_INPUT_EXT`. (3) The existing `_convert_document()` method uses pandoc which already supports DOC and RTF input — just add the MIME types to the dispatch block in `convert()` alongside the existing OOXML types. (4) Add `application/msword` and `application/rtf` to the `_categorize_mime()` function in `backend/app/services/ingestion.py` so they map to content_type `"document"`. (5) Verify pandoc in the backend Docker image supports DOC format (it uses LibreOffice as a backend for DOC — may need `libreoffice-writer` installed in the Dockerfile). If LibreOffice is too heavy, use `antiword` for DOC→text as a lighter alternative and document the trade-off.

- [x] D6.3: Re-process existing PDFs that were uploaded without text extraction -- The 3 PDFs already uploaded have no text extract (they were ingested before D6.1). Add a one-time migration/reprocessing endpoint: (1) Create `POST /api/admin/reprocess-sources` endpoint in a new `backend/app/routers/admin.py`. (2) The endpoint queries all Source records where `text_extract_encrypted IS NULL` and `mime_type = 'application/pdf'` (or any newly-supported type). (3) For each, retrieve the original file from the vault, run it through the updated preservation service to extract text, encrypt the text extract, update the Source record with `text_extract_encrypted` and `text_extract_dek`, and trigger search token generation + embedding for the new text. (4) Return a summary of how many sources were reprocessed. (5) Add a "Reprocess" button in the Settings page that calls this endpoint with a progress indicator.

- [x] D6.4: Add inline document viewer to MemoryDetail for PDFs and Office documents -- When viewing a document-type memory in `frontend/src/components/MemoryDetail.tsx`, the user only sees extracted text — there is no way to view the actual file. Fix: (1) In the `useEffect` that fetches vault files, expand the condition from `content_type === "photo"` to also handle `"document"`. (2) For PDFs (`mime_type === "application/pdf"` or preserved format is PDF): fetch the vault file via `fetchVaultFile()`, create an object URL, and render it in an `<iframe>` or `<object>` tag with `type="application/pdf"` — browsers have built-in PDF viewers that handle this natively. Style the viewer with a fixed height (e.g. `h-[600px]`) and full width, with a rounded border matching the app theme. (3) For DOCX/DOC files that were converted to PDF by the preservation service: fetch the *preserved* copy (which is the PDF) from the vault rather than the original, and render it in the same PDF viewer. This requires adding a `fetchPreservedVaultFile(sourceId)` API function if one doesn't exist, or using the existing vault endpoint with a query param to select the preserved copy. (4) Add a "Download Original" button below the viewer that lets the user download the original file (DOC/DOCX/PDF) to their device. (5) Add a fallback message if the browser cannot render the PDF inline: "Your browser doesn't support inline PDF viewing" with a download link. (6) Keep the existing text extract display below the viewer — it serves as a searchable/readable version of the content.

- [x] D6.5: Add OCR text extraction for scanned/image-only PDFs and photos -- D6.1 extracts text from PDFs using pdfplumber, but scanned PDFs (which are just embedded images) and photos of documents produce empty or no text extracts, making them unsearchable. Fix: (1) Add `pytesseract` to `backend/requirements.txt` and install `tesseract-ocr` plus language packs (`tesseract-ocr-eng` at minimum) in the backend Dockerfile via `apt-get`. (2) In `backend/app/services/preservation.py`, add an `_ocr_extract_text()` method that takes image bytes (PIL Image), runs `pytesseract.image_to_string()`, and returns the extracted text (stripped/cleaned). (3) In the PDF handler (added by D6.1): after pdfplumber text extraction, if the result is empty or near-empty (e.g. less than 50 characters total across all pages), fall back to OCR — render each PDF page to an image using `pdfplumber`'s `.to_image()` or `pdf2image`/`poppler` and run tesseract on each page image, concatenating the results. Add `pdf2image` to requirements.txt and `poppler-utils` to the Dockerfile if using pdf2image for rendering. (4) For photo-type memories (`content_type === "photo"`): after image preservation (conversion to PNG), run OCR on the preserved image. If OCR produces meaningful text (more than ~20 characters), store it as `text_extract` so photos of documents, receipts, whiteboards, etc. become searchable. (5) Add a config flag `OCR_ENABLED=true` in `.env.example` and `backend/app/config.py` so OCR can be disabled if tesseract is not installed or the user doesn't want the overhead. (6) OCR can be slow — log processing time and consider running it in the background worker rather than inline during ingest if it exceeds a threshold (e.g. 10 seconds). (7) Update the D6.3 reprocessing endpoint to also re-process photos and image-only PDFs that have no text extract, triggering OCR on them.

## Discovery Round 7

- [x] D7.1: Add server-side session timeout to wipe KEK from memory after inactivity -- ARCHITECTURE.md §6.5 specifies "After configurable timeout (default 15 minutes), KEK is wiped from server memory", but `backend/app/auth_state.py` has no timeout logic. Master keys stored via `store_master_key()` persist indefinitely until explicit logout or server restart. Fix: (1) Add a `last_activity: datetime` timestamp to each entry in `_active_sessions` (change the dict value from `bytearray` to a small dataclass holding `key: bytearray` and `last_activity: datetime`). (2) Update `get_master_key()` to check `last_activity` against a configurable `SESSION_TIMEOUT_MINUTES` (default 15, from Settings). If expired, call `_secure_zero()` on the key, remove the entry, and return None. (3) Update `get_master_key()` to refresh `last_activity` on each successful access (sliding window). (4) Add `SESSION_TIMEOUT_MINUTES` to `backend/app/config.py` and `.env.example`. (5) The existing `require_auth` dependency in `dependencies.py` already handles None keys by returning 401, so no router changes needed.

- [x] D7.2: Implement data export endpoint for full brain takeout -- ARCHITECTURE.md §4 specifies `routers/export.py` for "Data export/migration" but the file does not exist and there is no export functionality anywhere in the codebase. Users cannot export their data, which contradicts the project's anti-vendor-lock-in philosophy. Fix: (1) Create `backend/app/routers/export.py` with `POST /api/export` endpoint. (2) The endpoint should generate a portable archive containing: all memories as decrypted Markdown files (one per memory), all vault files decrypted to their original format, a `metadata.json` with memory metadata/connections/tags, and a `README.txt` explaining the export format. (3) Stream the archive as a ZIP file (using `zipfile` in-memory or `StreamingResponse`). (4) Require auth (the KEK must be active to decrypt). (5) Register the router in `main.py`. (6) Add an "Export All Data" button in `frontend/src/components/Settings.tsx` that calls this endpoint and triggers a browser download.

- [x] D7.3: Add vault-wide integrity verification as a scheduled health check -- ARCHITECTURE.md §7.4 specifies a nightly cron job that verifies: (a) every Source record has a corresponding `.age` file on disk, (b) no orphan `.age` files exist without a Source record, (c) SHA-256 hashes match. Currently `VaultService.verify_integrity()` only checks a single file on retrieval. Fix: (1) Add a `verify_all(session: Session) -> dict` method to `VaultService` that queries all Source records, checks each vault_path exists on disk, optionally spot-checks hashes (full check is expensive — do 10% random sample by default), scans vault directories for orphan .age files, and returns a summary dict with counts of missing/orphaned/corrupted files. (2) Add a `GET /api/health/vault` endpoint in `routers/health.py` that runs this check and returns the result. (3) Hook it into the background worker as a periodic job (daily), logging results and surfacing failures via the health endpoint.

- [x] D7.4: Create root README.md with quick-start guide -- ARCHITECTURE.md §4 line 194 specifies `README.md — Quick start guide` but no README.md exists at the project root. New users or future maintainers have no entry point. Fix: Create `README.md` with: (1) One-line project description. (2) Prerequisites (Docker, Docker Compose). (3) Quick start: `cp .env.example .env && scripts/init.sh && docker compose up -d`. (4) Links to ARCHITECTURE.md for full specs and RECOVERY.md for disaster recovery. (5) Security warning about passphrase = sole access, no recovery without Shamir shares. (6) License info.

- [x] D7.5: Add missing test coverage for chat, admin, and backup routers -- `backend/tests/` has no `test_chat.py`, `test_admin.py`, or `test_backup_router.py`. The chat endpoint (`routers/chat.py`) handles WebSocket RAG streaming, the admin endpoint (`routers/admin.py`) handles source reprocessing, and the backup endpoint (`routers/backup.py`) handles backup status — all untested at the API level. Fix: (1) Create `backend/tests/test_chat.py` with tests for WebSocket connection, message send/receive, and auth requirement. (2) Create `backend/tests/test_admin.py` with tests for the reprocess endpoint (mock vault retrieval and preservation). (3) Create `backend/tests/test_backup_router.py` with tests for backup status endpoint. Use existing `conftest.py` fixtures and mock external services (Ollama, Qdrant).

- [x] D7.6: Add `img-src data:` to Content-Security-Policy in Caddyfile -- The CSP header in `Caddyfile` line 37 sets `img-src 'self' blob:` but the frontend may render base64-encoded image thumbnails or inline SVGs using `data:` URIs (e.g., the ErrorBoundary component, or future avatar/icon features). Missing `data:` in `img-src` will cause silent image loading failures in production behind Caddy. Fix: Update the CSP `img-src` directive from `img-src 'self' blob:` to `img-src 'self' blob: data:` in the Caddyfile.

## Discovery Round 8

- [x] D8.1: Clickable tag chips on Timeline cards filter memories by tag -- Clicking a tag chip on a Timeline memory card should filter the Timeline to show only memories with that tag. The backend `GET /api/memories` already supports `tag_ids` query param (AND logic), and the frontend `listMemories()` in `api.ts` already accepts `tag_ids`. Fix: (1) In `Timeline.tsx`, import `useSearchParams` from react-router-dom. (2) Read `tag` and `tagName` from URL search params on mount (e.g. `/timeline?tag=<uuid>&tagName=Travel`). Store as `selectedTagId` / `selectedTagName` state, initialized from URL params. (3) Pass `tag_ids: selectedTagId ? [selectedTagId] : undefined` to `listMemories()` calls in `loadInitial()` and `loadMore()`. (4) Add `selectedTagId` to the `useEffect` dependency array alongside `selectedYear` so changing tag triggers a reload. (5) When a tag filter is active, show a filter banner below the TimelineBar: `"Filtered by tag: {tagName}"` with a "Clear" button that clears the URL params and resets state. (6) Make tag chips on memory cards clickable — wrap each `<span>` tag chip in the memory card with an `onClick` handler that calls `navigate(`/timeline?tag=${tag.tag_id}&tagName=${encodeURIComponent(tag.tag_name)}`)`. Use `e.preventDefault()` and `e.stopPropagation()` to prevent the parent `<Link>` from navigating to the memory detail page. (7) The year filter and tag filter should work together (both applied simultaneously). (8) Clear the tag filter when a year is selected from the TimelineBar (to avoid confusing combined filters), or keep both — user preference is to keep both active simultaneously.

## Phase 7: Memory Card Actions

- [x] P7.1: Add visibility field to Memory model -- Add `visibility: str = Field(default="public")` to the Memory SQLModel in `backend/app/models/memory.py`. Add `visibility` to `MemoryUpdate` and `MemoryRead` schemas. Run alembic or manual `ALTER TABLE memories ADD COLUMN visibility TEXT DEFAULT 'public'` via a startup migration in `db.py`. In `backend/app/routers/memories.py`, update the list endpoint to accept `?visibility=public|private|all` query param (default `public` — only returns public memories unless explicitly requested). Update the PUT endpoint to allow changing `visibility`. Add `visibility` to the frontend `Memory` type in `frontend/src/types/index.ts` and to the `listMemories` params in `frontend/src/services/api.ts`.

- [x] P7.2: Add cascade delete for vault files and embeddings on memory deletion -- The existing `DELETE /api/memories/{id}` in `backend/app/routers/memories.py` deletes DB records (memory, source, connections, search tokens) but does NOT delete the vault `.age` file from disk or remove vectors from Qdrant. Fix: (1) Before deleting the Source record, read its `vault_path` and `preserved_vault_path`, then delete those files from `data/vault/` via `VaultService`. (2) After deleting the memory record, remove its vectors from Qdrant by calling `embedding_service.delete_by_memory_id(memory_id)` — add this method to `backend/app/services/embedding.py` if it doesn't exist (it should call `qdrant_client.delete(collection, points_selector=FilterSelector(filter=Filter(must=[FieldCondition(key="memory_id", match=MatchValue(value=memory_id))])))`. (3) Add proper error handling so a vault deletion failure doesn't prevent the DB cleanup.

- [x] P7.3: Create MemoryCardMenu component with three-dot context menu -- Create `frontend/src/components/MemoryCardMenu.tsx` — a three-dot button (`...`) that opens a dropdown menu with actions: Edit (navigates to `/memory/{id}/edit` or `/memory/{id}` with edit mode), Delete (opens confirmation modal), and Toggle Visibility (shows "Make Private" or "Make Public" depending on current state). The component receives `memoryId`, `visibility`, `onDelete`, and `onVisibilityChange` as props. Use absolute positioning for the dropdown, close on outside click. Style to match the dark theme (bg-gray-800 dropdown, hover states). Render this component on each timeline card in `Timeline.tsx` (top-right corner, stopping event propagation so it doesn't navigate to the memory detail). Also render it on `MemoryDetail.tsx` in the header area.

- [x] P7.4: Wire up delete and visibility actions in Timeline and MemoryDetail -- In `frontend/src/components/Timeline.tsx`: (1) Add a `handleDelete(memoryId)` function that calls `DELETE /api/memories/{id}` via api.ts (add `deleteMemory` function to api.ts if missing), then removes the memory from local state and refreshes stats. (2) Add a `handleVisibilityChange(memoryId, visibility)` function that calls `PUT /api/memories/{id}` with the new visibility, updates local state. (3) Add a confirmation modal component (simple dialog with "Are you sure?" + Cancel/Delete buttons) that appears before delete. (4) In `MemoryDetail.tsx`, add the same delete action (navigates back to `/timeline` after successful delete) and visibility toggle. (5) Add a "Show private" toggle to the Timeline header that passes `?visibility=all` to the list endpoint, with a visual indicator (e.g. eye icon) when private memories are visible.

## Phase 8: On This Day Carousel

- [x] P8.1: Create On This Day backend endpoint -- Add `GET /api/memories/on-this-day` to `backend/app/routers/memories.py`. The endpoint queries memories where the month and day of `captured_at` match today but the year is earlier than the current year. Use SQLite's `strftime('%m', captured_at)` and `strftime('%d', captured_at)` for date extraction. Return up to 10 memories ordered by year descending (most recent first). Each result includes the standard `MemoryRead` fields. Requires auth. If no memories match, return an empty list (the frontend hides the carousel when empty).

- [x] P8.2: Add LLM engagement prompt generation for On This Day -- Create `GET /api/memories/{id}/reflect` in `backend/app/routers/memories.py` (or a new `backend/app/routers/reflect.py`). Given a memory ID, decrypt the memory content, pass it to the LLM via the existing `llm.py` service with a system prompt: "Given this memory from {year}, generate a single short question (under 15 words) that invites the user to reflect on it. Be warm, personal, and specific to the content." Return `{ "prompt": "..." }`. Cache the result for 24 hours (store in a simple `reflection_prompts` table with `memory_id`, `prompt_text`, `generated_at`, or use an in-memory TTL cache). This endpoint is optional — if it fails or LLM is unavailable, the frontend falls back to generic prompts.

- [x] P8.3: Create OnThisDay frontend carousel component -- Create `frontend/src/components/OnThisDay.tsx`. On mount, fetch `GET /api/memories/on-this-day` (add `getOnThisDayMemories()` to api.ts). If the response is empty, render nothing. Otherwise render a horizontal scrollable container with CSS scroll-snap (`overflow-x-auto snap-x snap-mandatory`) showing memory cards: each card displays the year ("3 years ago"), the memory title (decrypted), a thumbnail if it's a photo, and an engagement prompt. For the prompt: try fetching `/api/memories/{id}/reflect` for each memory, fall back to generic prompts ("How do you feel about this now?", "Would you add anything?", "What's changed since then?") if the endpoint fails. Add a "Respond" button that opens QuickCapture pre-filled with context (pass a `prefill` prop). Add a dismiss button (X) that hides the carousel for the session (store in `sessionStorage`). Props: `{ onMemoryCreated: () => void }`.

- [x] P8.4: Integrate OnThisDay carousel into Timeline page -- In `frontend/src/components/Timeline.tsx`, import and render `<OnThisDay onMemoryCreated={handleRefresh} />` between the Timeline header and the TimelineBar. Only render it when no year filter or tag filter is active (it's a "today" feature, not relevant when browsing filtered views). The carousel should be the first visual element after the page title, making it the first thing users see — just like Facebook's "Memories" section.

## Phase 9: Sidebar Filters

- [x] P9.1: Extend memories list API with compound filter params -- In `backend/app/routers/memories.py`, add query parameters to the list endpoint: `content_type: str | None` (comma-separated, e.g. "photo,text"), `date_from: str | None` (ISO date), `date_to: str | None` (ISO date), `visibility: str = "public"` (public|private|all). Build the SQL WHERE clause dynamically, combining all active filters with AND logic. The endpoint already supports `year` and `tag_ids` — these new params stack with them. Update `frontend/src/services/api.ts` `listMemories()` to accept and forward these new params.

- [x] P9.2: Create collapsible FilterPanel component in sidebar -- Create `frontend/src/components/FilterPanel.tsx`. Renders in the Layout sidebar below navigation and above "Lock & Logout". Contains collapsible sections: (1) **Content Type** — checkboxes for text, photo, file, voice, url. (2) **Date Range** — two date inputs (from/to). (3) **Tags** — multi-select dropdown using existing tag list from `listTags()`. (4) **Visibility** — radio buttons for All/Public/Private. Each section is collapsible (click header to toggle). The component maintains filter state and exposes it via an `onFilterChange(filters)` callback. Style: compact, dark theme matching sidebar. On mobile (below `md` breakpoint), render as a slide-up sheet triggered by a "Filters" button above the timeline.

- [x] P9.3: Integrate FilterPanel with Timeline data loading -- In `frontend/src/components/Layout.tsx`, render `<FilterPanel>` in the sidebar. Lift filter state to Layout or use URL search params (preferred — makes filters bookmarkable/shareable). In `Timeline.tsx`, read filter params from the URL and pass them to `listMemories()`. Show active filters as removable chips between the TimelineBar and QuickCapture (e.g. "Content: photo ✕ | Tags: Travel ✕ | Date: Jan–Mar 2024 ✕"). Clearing all filters resets the URL params. The existing year bar and tag chip filters should integrate with this system — selecting a year updates the URL `date_from`/`date_to` params, clicking a tag chip adds to `tag_ids`.

## Phase 10: Background AI Loops

- [x] P10.1: Create Suggestion model and LoopScheduler for background AI jobs -- Create `backend/app/models/suggestion.py` with a `Suggestion` SQLModel: `id`, `memory_id` (FK), `suggestion_type` (tag_suggest|enrich_prompt|digest|pattern), `content_encrypted`, `content_dek`, `status` (pending|accepted|dismissed), `created_at`, `updated_at`. Create `backend/app/services/loop_scheduler.py` with a `LoopScheduler` class that tracks when each loop type last ran (store in a `loop_state` table: `loop_name`, `last_run_at`, `next_run_at`, `enabled`). The scheduler checks on each worker cycle whether any loop is due and submits the appropriate job. Register the scheduler in the existing worker startup in `main.py` lifespan. Add loop config to `backend/app/config.py`: `TAG_SUGGEST_INTERVAL_HOURS=24`, `ENRICH_INTERVAL_HOURS=24`, `CONNECTION_RESCAN_INTERVAL_HOURS=6`, `DIGEST_INTERVAL_HOURS=168` (weekly).

- [x] P10.2: Implement tag suggestion loop -- Add a `TAG_SUGGEST` job type to `backend/app/worker.py`. The job queries memories that have zero tags, takes each memory's decrypted content, sends it to the LLM with a prompt: "Given this memory, suggest 1-3 short tags (single words or two-word phrases) that categorize it. Return only the tag names, one per line." Parse the LLM response into tag names. For each suggested tag, create a `Suggestion` record with `suggestion_type="tag_suggest"` and `content_encrypted` containing the suggested tag name (encrypted). The `LoopScheduler` triggers this daily. On ingest, also submit a `TAG_SUGGEST` job for the newly created memory (in addition to the existing embedding job). Existing tags are NOT duplicated — check against existing tags for the memory.

- [x] P10.3: Implement memory enrichment prompt loop -- Add an `ENRICH_PROMPT` job type to `backend/app/worker.py`. The job queries memories where `content` length (decrypted) is below a threshold (e.g., under 100 characters) or where no connections exist. For each, send the memory title and content to the LLM: "This memory seems brief. Generate a single thoughtful question (under 20 words) that would help the owner add more detail." Create a `Suggestion` record with `suggestion_type="enrich_prompt"`. The `LoopScheduler` triggers this daily. Limit to 5 suggestions per cycle to avoid overwhelming the user.

- [x] P10.4: Create suggestions API endpoints -- Create `backend/app/routers/suggestions.py` with: (1) `GET /api/suggestions` — list pending suggestions (status=pending), paginated, most recent first. Returns decrypted suggestion content. (2) `POST /api/suggestions/{id}/accept` — marks suggestion as accepted and applies it (for tag suggestions: creates the tag if needed and adds it to the memory; for enrichment: no automatic action, just marks accepted). (3) `POST /api/suggestions/{id}/dismiss` — marks suggestion as dismissed. Register the router in `main.py`. Add `getSuggestions`, `acceptSuggestion`, `dismissSuggestion` to `frontend/src/services/api.ts`.

- [x] P10.5: Create suggestions UI cards in Timeline -- Create `frontend/src/components/SuggestionCard.tsx` — a distinct card style (e.g., dashed border, subtle blue/purple accent) that renders inline in the Timeline feed. For tag suggestions: show "AI suggests tagging '{memory_title}' with: {tags}" with Accept/Dismiss buttons. For enrichment prompts: show the LLM-generated question with a "Respond" button that opens QuickCapture. In `Timeline.tsx`, fetch suggestions via `getSuggestions()` and interleave them at the top of the memory list (above actual memories, below QuickCapture). Limit to 3 visible suggestions. Add a toggle in `frontend/src/components/Settings.tsx` to enable/disable AI suggestions and configure loop cadences (calls a new settings endpoint or updates `.env`).

## Phase 11: Immich Integration & People

- [ ] P11.1: Create Person model and MemoryPerson join table -- Create `backend/app/models/person.py` with: `Person` SQLModel (`id`, `name`, `name_encrypted`, `name_dek`, `immich_person_id` nullable for Immich sync, `face_thumbnail_path` nullable, `created_at`, `updated_at`) and `MemoryPerson` join table (`id`, `memory_id` FK, `person_id` FK, `source` = "manual"|"immich"|"auto", `confidence` float nullable, `created_at`). Add relationships so a Memory can have many Persons and vice versa. Create `backend/app/routers/persons.py` with CRUD: `POST /api/persons` (create), `GET /api/persons` (list all), `GET /api/persons/{id}` (detail + linked memories), `PUT /api/persons/{id}` (update name), `DELETE /api/persons/{id}`. Add `POST /api/memories/{id}/persons` to link a person to a memory and `DELETE /api/memories/{id}/persons/{person_id}` to unlink. Register in `main.py`.

- [ ] P11.2: Create Immich sync service -- Create `backend/app/services/immich.py` with an `ImmichService` class. Config: `IMMICH_URL` and `IMMICH_API_KEY` in `backend/app/config.py` and `.env.example`. Methods: (1) `sync_people()` — call Immich `GET /api/people` to fetch all known people with face thumbnails, upsert into the local Person table matching on `immich_person_id`. (2) `sync_faces_for_asset(asset_id)` — call Immich `GET /api/faces?id={asset_id}` to get detected faces for a photo, create MemoryPerson links. (3) `push_person_name(person_id, name)` — call Immich `PUT /api/people/{id}` to push a name tagged in Mnemos back to Immich. Add a `IMMICH_SYNC` job type to the worker that runs the sync periodically (configurable, default every 6 hours). Only activate if `IMMICH_URL` is configured.

- [ ] P11.3: Create face tagging UI and people filter -- Create `frontend/src/components/People.tsx` as a new route `/people`. Shows a grid of all known persons (name + face thumbnail). Click a person to see all their linked memories. Add an "Untagged" section showing face thumbnails from Immich that haven't been assigned a name — user clicks to assign. Create a `FaceTagModal` component: shows the face crop, a text input to type/search a person name, and Save/Skip buttons. In `frontend/src/components/Layout.tsx`, add "People" to the nav items. In the sidebar `FilterPanel` (from P9.2), add a People section that lists persons as checkboxes — checking a person filters the timeline to memories containing that person. Update `listMemories` in api.ts to accept `person_ids` param and the backend list endpoint to filter by it.

## Phase 12: Location & Geo-Tagging

- [ ] P12.1: Add location fields to Memory model and EXIF extraction on ingest -- Add `latitude: float | None`, `longitude: float | None`, `place_name: str | None` (encrypted), `place_name_dek: str | None` fields to the Memory model in `backend/app/models/memory.py` and to `MemoryRead`/`MemoryUpdate` schemas. In `backend/app/services/ingestion.py`, after detecting a photo content type, extract GPS coordinates from EXIF data using Pillow's `Image.getexif()` and the GPSInfo tag. Store latitude/longitude on the Memory record. Add `place_name` to `MemoryUpdate` so users can manually set location on any memory type. Update the frontend `Memory` type in `types/index.ts`.

- [ ] P12.2: Add reverse geocoding and location-based query support -- Create `backend/app/services/geocoding.py` with a `GeocodingService` class. Use Nominatim (OpenStreetMap) via `httpx` for reverse geocoding: given lat/lng, return a place name (city, country). Call this during photo ingest after extracting GPS coords, encrypt the result, store as `place_name`/`place_name_dek` on the Memory. Add rate limiting (1 req/sec for Nominatim ToS). Add `near` query param to `GET /api/memories`: `?near=lat,lng,radius_km` that filters using the Haversine formula in SQL (`acos(sin(radians(lat1))*sin(radians(lat2))+cos(radians(lat1))*cos(radians(lat2))*cos(radians(lng2-lng1)))*6371 < radius_km`). Add `GEOCODING_ENABLED=true` to config.

- [ ] P12.3: Create map view page with Leaflet -- Add `leaflet` and `react-leaflet` to `frontend/package.json`. Create `frontend/src/components/MapView.tsx` as a new route `/map`. Fetch all memories that have latitude/longitude set (add `?has_location=true` param to the list endpoint). Render a Leaflet map with markers at each memory's location. Clicking a marker shows a popup with the memory title, date, and a link to the full memory. Cluster nearby markers using `react-leaflet-cluster` to handle many pins. Add "Map" to the nav items in `Layout.tsx`. Style the map container to fill the main content area. Use OpenStreetMap tiles (free, no API key needed).

- [ ] P12.4: Add location display and manual location tagging to memory UI -- In `frontend/src/components/MemoryDetail.tsx`, if the memory has lat/lng, show a small static map (Leaflet, 200px height) with a pin and the place name below it. Add an "Add Location" button for memories without coordinates — opens a modal with a Leaflet map where the user can click to set a pin, plus a text search box that geocodes place names to coordinates (call Nominatim from the frontend). Saving the location calls `PUT /api/memories/{id}` with the new lat/lng/place_name. In the sidebar `FilterPanel`, add a Location section with a text input — typing a place name filters to memories near that location (using the `near` query param).
